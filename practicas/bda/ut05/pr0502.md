```
------------- ESPECIALIZACIÓN EN INTELIGENCIA ARTIFICIAL Y BIG DATA -------------
---------------------------------------------------------------------------------

Módulo:                     BIG DATA APLICADO
Profesor:                   Víctor J. González
Unidad de Trabajo:          UT05. Procesamiento distribuido con PySpark
Práctica:                   PR0502. Manipulación básica de dataframes
Resultados de aprendizaje:  RA1
```

# PR0502. Manipulación básica de dataframes

Vamos a empezar a realizar operaciones básicas sobre los datasets que cargamos en la anterior práctica.

## Dataset 1: Datos para la predicción del rendimiento en cultivos

![Dataset cultivos](images/crops.png)

### 1.- Selección de características

Crea un nuevo DataFrame llamado `df_sel` que contenga únicamente las columnas: `Crop`, `Region`, `Temperature_C`, `Rainfall_mm`, `Irrigation` y `Yield_ton_per_ha`.

### 2.- Normalización de nombres

Los nombres actuales son muy largos y técnicos (tienen unidades). Necesitamos estandarizarlos al español o simplificarlos. Usando el DataFrame `df_sel` del ejercicio anterior, cambia los siguientes nombres:

- `Temperature_C` -> `Temperatura`
- `Rainfall_mm` -> `Lluvia`
- `Yield_ton_per_ha` -> `Rendimiento`

Guarda el resultado en `df_renamed`.

### 3.- Filtrado de datos (`filter`)

Supón que queremos centrarnos en cultivos de **maíz** que han crecido en regiones calurosas (más de 25 grados). Filtra `df_renamed` para mantener solo las filas donde:

- El cultivo (`Crop`) sea igual a "Maize".
- La temperatura (`Temperatura`) sea mayor a 25.

### 4.- Encadenamiento

En Spark podemos **encadenar** varias funciones. Repite las órdenes anteriores encadenadas en una única sentencia

## Dataset 2: Lugares famosos del mundo

![Dataset lugares famosos](images/places.png)

### 1.- Selección de datos críticos

El dataset tiene mucha información descriptiva que no necesitamos para el análisis cuantitativo (como `Famous_For` o `Best_Visit_Month`).

Crea un nuevo DataFrame llamado `df_base` seleccionando únicamente: `Place_Name`, `Country`, `UNESCO_World_Heritage`, `Entry_Fee_USD` y `Annual_Visitors_Millions`.

### 2.- Traducción y simplificación

Las columnas tienen nombres en inglés y son demasiado largos para los reportes en español. Sobre el DataFrame `df_base`, renombra las columnas de la siguiente manera y guarda el resultado en `df_es`:

- `Place_Name` -> `Lugar`
- `UNESCO_World_Heritage` -> `Es_UNESCO`
- `Entry_Fee_USD` -> `Precio_Entrada`
- `Annual_Visitors_Millions` -> `Visitantes_Millones`

### 3: Filtrado

Supón que vamos a realizar una campaña y necesitamos filtrar los destinos que cumplan dos condiciones estrictas. Filtra `df_es` para obtener solo los registros que cumplan:

1. Sean Patrimonio de la Humanidad (`Es_UNESCO` es igual a "Yes").
2. El precio de entrada (`Precio_Entrada`) sea menor o igual a **20 dólares**.


## Dataset 3: Registro turístico de Castilla y León

![Dataset registro turístico](images/turismo.png)

### 1.- Selección y saneamiento

Las columnas originales como `N.Registro` o `GPS.Latitud` tienen puntos, lo cual suele dar problemas en motores SQL o al guardar en Parquet. Además, solo necesitamos datos de contacto.

Crea un `df_contactos` seleccionando únicamente: `Nombre`, `Tipo`, `Provincia`, `web` y `Email`.

### 2.- Renombrado estándar

Vamos a renombrar las columnas para que estén en minúsculas y no tengan ambigüedades

- `Nombre`  `nombre_establecimiento`
- `Tipo`  `categoria_actividad`
- `web`  `sitio_web`
- `Email`  `correo_electronico`

Guarda el resultado en `df_limpio`.

### 3: Filtrado de texto

La columna `categoria_actividad` contiene valores sucios como *"g - Bodegas y los complejos de enoturismo"*. No podemos filtrar por igualdad exacta (`==`).

Filtra `df_limpio` para obtener una lista `df_final` que cumpla **todas** estas condiciones simultáneamente:

1. **Ubicación:** la `Provincia` debe ser "Burgos".
2. **Actividad:** la `categoria_actividad` debe contener la palabra "Bodegas" (investiga [`contains`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.contains.html)] o [`like`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.like.html)).
3. **Digitalización:** el `sitio_web` **no** puede estar vacío ni ser nulo (investiga la función [`isNotNull`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.isNotNull.html)).


