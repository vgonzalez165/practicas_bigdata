```
------------- ESPECIALIZACIÓN EN INTELIGENCIA ARTIFICIAL Y BIG DATA -------------
---------------------------------------------------------------------------------

Módulo:                     BIG DATA APLICADO
Profesor:                   Víctor J. González
Unidad de Trabajo:          UT05. Procesamiento distribuido con PySpark
Práctica:                   PR0503. Limpieza de datos sobre dataset de cultivos
Resultados de aprendizaje:  RA1
```

# PR0503. Limpieza de datos sobre dataset de cultivos

Seguimos avanzando en el conocimiento de PySpark realizando tareas más avanzadas.

## Dataset 1: Datos para la predicción del rendimiento en cultivos

![Dataset cultivos](images/crops.png)

Supón que queremos preparar los datos de nuestro dataset de cultivos para un modelo de redes neuronales y nos han pedido cuatro transformaciones específicas:

- Generar identificadores únicos estandarizados
- Normalizar las distribuciones numéricas
- Comparar insumos
- Proyectar fechas de cosecha

### 1.- Creación de un ID único

Necesitamos un código único para cada registro que sirva como clave primaria. Crea una nueva columna llamada `Crop_ID` en un nuevo DataFrame `df_eng`. Este ID debe seguir este formato estricto: `CODIGO_REGION-CULTIVO`.

- **Limpieza:** de la columna `Region` (ej. "Region_C"), elimina la palabra "Region_" y quédate solo con la letra (puedes usar [`substring`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.substring.html) o [`split`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.split.html)).
- **Formato:** convierte el nombre del cultivo (`Crop`) a mayúsculas ([`upper`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.upper.html)).
- **Concatenación:** une la letra de la región y el cultivo con un guion medio ([`concat_ws`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.concat_ws.html)).
- **Relleno:** si por algún motivo la letra de la región fuera muy corta (improbable aquí, pero por seguridad), asegúrate de que esa parte tenga al menos 3 caracteres rellenando con 'X' a la izquierda ([`lpad`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lpad.html)). *Nota: Como en este dataset es solo una letra, el lpad rellenará con dos X, ej: "XXC".*


### 2: Transformación matemática

Los valores de lluvia tienen mucha varianza y el rendimiento tiene demasiados decimales irrelevantes. Añade/Modifica las siguientes columnas en `df_eng`:

- `Log_Rainfall`: calcula el logaritmo natural ([`log`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.log.html)) de la columna `Rainfall_mm` + 1 (para evitar errores si hubiera un 0).
- `Yield_Redondeado`: redondea el rendimiento (`Yield_ton_per_ha`) a 1 solo decimal usando la función [`round`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.round.html).
- `Rendimiento_Bancario`: crea otra columna usando [`bround`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.bround.html) sobre el rendimiento (sin decimales) para comparar cómo redondea Spark.


### 3.- Comparación de insumos

Queremos saber cuál fue el insumo químico más pesado aplicado en cada parcela. Crea una columna llamada `Max_Quimico_kg`.

- Usa la función [`greatest`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.greatest.html) para comparar, fila por fila, el valor de `Fertilizer_Used_kg` contra `Pesticides_Used_kg`. El resultado debe ser el valor más alto de los dos.


### 4.- Simulación de fechas

El dataset original no tiene fecha, pero sabemos que todos estos datos corresponden a la siembra del **1 de Abril de 2023**.

- Crea una columna `Fecha_Siembra` usando [`to_date`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.to_date.html) sobre el literal "2023-04-01".
- Calcula la `Fecha_Estimada_Cosecha` sumando 150 días a la fecha de siembra ([`date_add`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.date_add.html)).
- Extrae el mes de la cosecha en una columna nueva llamada `Mes_Cosecha` ([`month`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.month.html)).
