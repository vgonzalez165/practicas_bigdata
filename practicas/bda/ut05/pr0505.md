```
------------- ESPECIALIZACIÓN EN INTELIGENCIA ARTIFICIAL Y BIG DATA -------------
---------------------------------------------------------------------------------

Módulo:                     BIG DATA APLICADO
Profesor:                   Víctor J. González
Unidad de Trabajo:          UT05. Procesamiento distribuido con PySpark
Práctica:                   PR0505. Análisis de estadísticas en dataset
Resultados de aprendizaje:  RA1
```

# PR0505. Análisis de estadísticas en dataset

En esta práctica trabajarás sobre [este dataset](https://www.kaggle.com/datasets/juhibhojani/house-price) que contiene precios de casas en la India. 

Dado que es un dataset local con formatos regionales (Rupias, Lakhs, Crores, pies cuadrados), deberás transformarlo a un estándar internacional (dólares USD y metros cuadrados) para poder realizar un análisis estadístico comparativo y preparar los *features* para el entrenamiento del modelo.

## 1. Objetivos de ingeniería de datos (ETL)

Antes de cualquier análisis, debe ejecutar un pipeline de transformación sobre los datos crudos.

### 1.1. Estandarización monetaria (de INR a USD)

Las columnas `Amount(in rupees)` y `Price (in rupees)` vienen en formato de texto con unidades indias (*Lac* y *Cr*).

1. **Limpieza:** convierte los textos a valores numéricos puros en rupias (INR). Ten en cuenta que 1 Lac = 100,000 INR y 1 Cr = 10,000,000 INR.


2. **Conversión:** crea una nueva columna `Amount_USD` convirtiendo las rupias a dólares. Asume que el valor de conversión es 1 INR = 0.012 USD.


### 1.2. Estandarización de superficie

La columna `Carpet Area` está en pies cuadrados (*sqft*).

1. **Limpieza:** elimina las unidades textuales y extrae el valor numérico.
2. **Conversión:** crea una nueva columna `Area_m2` convirtiendo los pies cuadrados a metros cuadrados. El factor de conversión es 1 sqft = 0.0929.


## 2. Objetivos de análisis estadístico

Utilizando las **nuevas columnas transformadas** (`Amount_USD` y `Area_m2`), calcula e interpreta las siguientes métricas de distribución usando PySpark:

### 2.1. Medidas de dispersión (varianza y desviación estándar)

- Calcula la varianza y la desviación estándar de la columna `Amount_USD`.
- **Pregunta:** Si la desviación estándar es muy alta en comparación con el precio medio (por ejemplo, si la media es $100k y la desviación es $80k), ¿podemos decir que el "precio promedio" es un buen representante del mercado? ¿O los precios son demasiado dispares para confiar en el promedio?


### 2.2. Medidas de Forma (Skewness y Kurtosis)

- Calcule el `skewness` (asimetría) y la `kurtosis` (curtosis) de `Amount_USD`.
- **Pregunta:** Suponiendo que has obtenido un valor positivo, ¿qué significa esto para el negocio? ¿Hay más oferta de casas "baratas" con algunas pocas mansiones ultra-caras, o hay muchas casas caras y pocas baratas?
- **Pregunta:** Supón que obtienes una kurtosis superior a 3. ¿Deberíamos preocuparnos por la presencia de datos erróneos o propiedades de lujo extremo que podrían distorsionar nuestros análisis futuros?


## 3. Interpretación para IA

Redacta un breve informe técnico respondiendo a estas situaciones de modelado:

### 3.1.- **Pre-procesamiento para redes neuronales:** 
 
Observando la *desviación estándar* de `Amount_USD` (precio) frente a la de `Area_m2` (superficie), notará que tienen escalas muy diferentes (miles de dólares vs. decenas de metros). Realiza los pasos necesarios para homogeneizar las escalas de ambas columnas.


### 3.2.- **Gestión de outliers (Kurtosis):**

Al calcular la kurtosis de la columna de precios (Amount_USD), puede que obtengas un valor muy alto (mayor a 3, e incluso superior a 10). Si no fuera así supondremos que lo es.

Esto indica una distribución "leptocúrtica" con "colas pesadas". En el contexto de Big Data, esto significa que unos pocos registros extremos (mansiones de lujo o errores de entrada) están distorsionando la media y la varianza, lo que podría arruinar el entrenamiento de futuros modelos de Machine Learning.

Debes **normalizar** la distribución aplicando una técnica de clipping eliminando los extremos superiores, para ello sigue estos pasos:

**Identifica el límite**

En entornos distribuidos, calcular un percentil exacto es computacionalmente muy costoso (requiere ordenar todos los datos). PySpark utiliza algoritmos de aproximación (como [Greenwald-Khanna](https://aakinshin.net/posts/greenwald-khanna-quantile-estimator/)). 

Vas a calcular el percentil 99 para eliminar todos los valores que lo superen. En PySpark se hace con el método [`.approxQuantile("Amount_USD", [0.99], 0.01)`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html). Este método devuelve una lista, por lo que deberás extraer el primer valor. El tercer parámetro (0.01) es la tolerancia de error permitida.

**Aplica el filtro**

Genera un nuevo DataFrame llamado `df_limpio` que excluya las propiedades que superen ese precio límite calculado (quédate con el 99% de los datos más "normales").

**Verificación**

Vuelve a calcular la kurtosis, la media y la stddev sobre este nuevo DataFrame `df_limpio`.

**Pregunta**: comparando los resultados antes y después del filtro: ¿Cuánto ha bajado la curtosis? ¿Ha cambiado drásticamente el precio medio al eliminar solo el 1% de los datos? Reflexiona sobre la sensibilidad de la media aritmética frente a los outliers en grandes volúmenes de datos.



## 4. Análisis de Segmentos (Grouping & Aggregation)

Algo que nos puede ocurrir al realizar el análisis estadístico sobre el dataset completo es que las métricas globales calculadas están "sucias" porque mezclan tipos de propiedades muy diferentes (no es justo comparar un estudio de 1 habitación con un ático de 4 habitaciones).

Vamos a refinar el análisis estadístico agrupando los datos por categorías clave.

### 4.1.- **Ingeniería de variable (Extracción de `BHK` - Bedroom-Hall-Kitchen):**

- La columna `Title` contiene información como *"1 BHK Ready to Occupy..."* o *"2 BHK..."*.
- Crea una nueva columna llamada `Num_Bedrooms` extrayendo el número antes de "BHK"


### 4.2.- **Cálculo de estadísticas por grupo:**

- Agrupa los datos por `Num_Bedrooms` (ej. 1 BHK, 2 BHK, 3 BHK).
- Para cada grupo, calcula simultáneamente: `Mean`, `StdDev` y `Skewness` del precio (`Amount_USD`).


### 4.3. **Preguntas de análisis para el modelo:**

Una vez calculadas las métricas por grupo (1 BHK, 2 BHK, etc.), responde a las siguientes preguntas sobre la estructura del mercado:

#### A. Análisis de variabilidad (Desviación Estándar):

Observa cómo cambia la desviación estándar de los precios a medida que aumentan las habitaciones. ¿La variabilidad se mantiene constante o se dispara en las propiedades más grandes?

**Pregunta**: Si la desviación es mucho mayor en los pisos de 3 BHK que en los de 1 BHK, ¿qué nos indica esto sobre la homogeneidad del producto? 

Pista: Un mercado con baja desviación sugiere que los inmuebles son muy parecidos entre sí (*commodities*). Una alta desviación sugiere que dentro de esa categoría conviven pisos "estándar" con propiedades de "ultra-lujo", haciendo que el mercado sea mucho más heterogéneo.


#### B. Confiabilidad del precio promedio:

**Pregunta**: Basándote en lo anterior, ¿en qué segmento (1 BHK o 3 BHK) dirías que el "Precio Promedio" es un indicador más fiable del valor real de una propiedad? Es decir, si tuvieras que tasar una propiedad "a ciegas" usando solo el promedio del mercado, ¿en qué tipo de apartamento tendrías más riesgo de equivocarte drásticamente por exceso o por defecto?


#### C. Detección de anomalías de mercado (Curtosis):

Compara la curtosis entre los apartamentos pequeños y los grandes.

**Pregunta**: ¿Qué segmento tiene una curtosis más alta (colas más pesadas)?

**Pregunta**: Si el segmento de 3 BHK tiene una curtosis muy elevada, significa que existen propiedades con precios desorbitados que rompen la norma. ¿Consideras que estas "mansiones" representan la realidad del barrio, o son excepciones que deberían analizarse en un estudio de mercado aparte para no distorsionar la visión general?