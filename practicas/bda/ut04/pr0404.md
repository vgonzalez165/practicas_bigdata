```
------------- ESPECIALIZACIÓN EN INTELIGENCIA ARTIFICIAL Y BIG DATA -------------
---------------------------------------------------------------------------------

Módulo:                     BIG DATA APLICADO
Profesor:                   Víctor J. González
Unidad de Trabajo:          UT04. Procesamiento distribuido con MapReduce
Práctica:                   PR0404. Aplicación de patrones MapReduce
Resultados de aprendizaje:  RA1
```

# PR0404: Aplicación de patrones MapReduce

## Descripción

En esta práctica procesaréis el dataset de PIB mundial disponible en [Kaggle](https://www.kaggle.com/datasets/fredericksalazar/pib-gdp-global-by-countries-since-1960-to-2021) y practicaremos en el uso de diferentes **patrones de diseño** de MapReduce.

## Dataset

El *dataset* tiene dos archivos, utilizaremos el llamado `countries_gdp_hist.csv`, que tiene los siguientes campos: 
- `country_code`: código de país
- `region_name`: nombre de la región
- `sub_region_name`: nombre de la subregión
- `intermediate_region`: región intermedia
- `country_name`: nombre de país
- `income_group`: grupo de ingresos
- `year`: año
- `total_gdp`: Producto Interior Bruto total
- `total_gdp_million`: Producto Interior Bruto en millones
- `gdp_variation`: variación del Producto Interior Bruto


## Ejercicio 1: Limpieza y Transformación

### Patrón

Filtrado y transformación.

### Objetivo

El *dataset* tiene años muy antiguos y valores nulos. Queremos un *dataset* limpio para años del siglo XXI.

### Implementación

1.  **Mapper**:
      - Lee línea por línea desde `sys.stdin`.
      - **Valida:** ignora cabeceras o líneas con errores de formato.
      - **Filtra:** conserva solo registros donde `year >= 2000` y `total_gdp > 0`.
      - **Transforma:** emite solo `Country Name`, `Year` y `Total GDP`.
      - **Salida:** imprime en STDOUT separado por tabuladores (`\t`).
2.  **Reducer:**
      - En este caso el reducer no tiene que hacer nada



## Ejercicio 2: Agregación por clave

### Patrón

Resumen numérico (promedio).

### Objetivo

Calcular el PIB promedio histórico por cada Región (Asia, Americas, Europe...).


### Implementación

1.  **Mapper**:
      - Extrae `region_name` y `total_gdp`.
      - Emite: `REGION \t GDP`
2.  **Reducer**:
      - Hadoop envía los datos ordenados por clave, hay que ir sumando los valores y el número de ocurrencias de la clave que llevamos. Cuando cambie la clave se emite el promedio de la región y se resetean contadores.
3.  **Salida esperada:** `AMERICAS  650000.50`, `EUROPE  540000.20`, etc.



## Ejercicio 3: Máximos por grupo (Filtering/Top-K)

### Patrón

Top-K per Group.

### Objetivo

Encontrar el año de mayor variación de PIB (`gdp_variation`) para cada país.

### Implementación

1.  **Mapper**:
      - Emite: `Country_Name \t Year,Variation` (Concatena año y variación en el valor para no perder el dato del año).
2.  **Reducer**:
      - Para cada país, recorre todas las variaciones.
      - Mantén en memoria solo la variación más alta encontrada hasta el momento y su año asociado.
      - Al cambiar de país, emite: `PAIS \t AÑO_RECORD (VARIACION)`



## Ejercicio 4: Join (Reduce-Side Join)

### Patrón

Reduce-Side Join.

### Objetivo

Unir el dataset de PIB con un dataset auxiliar de códigos de país para obtener el nombre completo en español (simulado).

### Implementación

1.  **Datos Auxiliares:** necesitas un fichero que relacione los códigos de los países con su respectivo nombre. Puedes utilizar [este *dataset*](https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes/blob/master/all/all.csv) disponible en Github.
2.  **Estrategia:**
      - Subir ambos archivos (`gdp.csv` y `codes.csv`) a HDFS.
      - **Mapper**
          - Debe detectar qué archivo está leyendo. Una forma de hacerlo sería contando la cantidad de columnas del fichero. Ten cuidado, porque en un fichero el separador es el punto y coma, mientras que en el otro es la coma, así que lo que puedes hacer es:
            - Separo por comas, si obtengo `X` campos es el fichero `codes.csv`
            - Si no, separo por punto y coma, y compruebo que el número de campos concuerda con el fichero `gdp.csv`
          - Si es `codes.csv`: Emite `CODIGO \t A_NombreEsp` (Tag 'A', que usaré en el *reducer* para distinguir entre uno y otro).
          - Si es `gdp.csv`: Emite `CODIGO \t B_PIB` (Tag 'B').
      - **Reducer**
          - Recibirá todas las líneas de un mismo código juntas (ej. `ESP`).
          - Guarda el nombre en español (Tag A) en una variable.
          - Cuando lleguen los datos del PIB (Tag B), imprime: `Nombre_Español \t PIB`.
3. **Ejecución**: ten en cuenta que, en este caso, el *mapper* recibirá dos ficheros, por lo que habría que invocarlo de la siguiente forma (observa que hay dos `-input`):

    ```bash
    # Ejemplo de ejecución
    mapred streaming \
        -files mapper_join.py,reducer_join.py \
        -input /hdfs/ruta/gdp.csv \
        -input /hdfs/ruta/codes.csv \
        -output /hdfs/ruta/salida_join \
        -mapper mapper_join.py \
        -reducer reducer_join.py
    ```



## Ejercicio 5: Distribución de Riqueza (Binning Pattern)

### Patrón

Binning (Categorización en cubos).

### Objetivo

Clasificar los registros en rangos de riqueza definidos manualmente para generar un histograma. En lugar de agrupar por una columna existente (como Región), debes crear tu propia clave de agrupación basada en lógica de negocio.

Queremos saber cuántos registros de la historia corresponden a economías "Pequeñas", "Medianas" y "Grandes" basándonos en el `total_gdp_million`.

Las **reglas de negocio (bins)** son:

  - **Economía Pequeña:** GDP \< 10,000 Millones.
  - **Economía Mediana:** 10,000 \<= GDP \< 1,000,000 Millones.
  - **Economía Grande:** GDP \>= 1,000,000 Millones.

### Implementación

1.  **Mapper**

      - Leer `total_gdp_million`.
      - Determinar la categoría según las reglas de negocio.
      - **Salida:** `CATEGORIA \t 1` (Emitimos un 1 para contar).

2.  **Reducer**

      - Suma simple de los "1" recibidos por cada categoría.
      - **Salida esperada:**
        ```text
        Economía Grande    250
        Economía Mediana   1500
        Economía Pequeña   3000
        ```


## Ejercicio 6: Índice invertido de países (Inverted Index Pattern)

### Patrón

Inverted Index (con deduplicación).

### Objetivo

Generar una lista de búsqueda rápida. Dado un nivel de ingresos (`income_group`), queremos obtener la lista de todos los países únicos que pertenecen a ese grupo.

El *dataset* es una serie temporal. El par `(INGRESO ALTO, ESPAÑA)` aparece unas 60 veces (una vez por cada año desde 1960). El *reducer* debe ser capaz de eliminar duplicados para no listar "España" 60 veces.

### Implementación

1.  **Mapper**

      - Leer `income_group` y `country_name`.
      - **Salida:** `INCOME_GROUP \t COUNTRY_NAME`

2.  **Reducer**

      - Recibir la lista de países para un grupo.
      - Almacenar los países en una estructura que no admita duplicados (como un `set` de Python) mientras se itera sobre la misma clave.
      - Al cambiar de clave, unir el set en un string separado por comas.
      - **Salida esperada:**
        ```text
        INGRESO ALTO    ARUBA, ESPAÑA, FRANCIA, ...
        INGRESO BAJO    AFGANISTÁN, ...
        ```

