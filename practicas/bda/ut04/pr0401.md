```
------------- ESPECIALIZACIÓN EN INTELIGENCIA ARTIFICIAL Y BIG DATA -------------
---------------------------------------------------------------------------------

Módulo:                     BIG DATA APLICADO
Profesor:                   Víctor J. González
Unidad de Trabajo:          UT02. HDFS. Almacenamiento distribuido
Práctica:                   PR0201. Instalación de Hadoop en modo pseudo-distribuido
Resultados de aprendizaje:  RA1, RA2 y RA3
```

# PR0401: MapReduce

## Objetivo


Implementa un trabajo (job) de MapReduce utilizando Python para procesar una gran obra literaria y obtener un recuento detallado de la frecuencia de cada palabra. El objetivo es ir más allá del simple conteo, aplicando limpieza y normalización de datos, un paso fundamental en el procesamiento de texto.

El problema de "contar palabras" (Word Count) es el "Hola, Mundo" del paradigma MapReduce. Sin embargo, en un escenario real, los datos de texto (como libros, artículos o logs) están "sucios". Contienen mayúsculas, minúsculas, signos de puntuación y palabras comunes (stopwords) que pueden distorsionar un análisis simple.

En esta práctica, analizarás el texto completo de **"El ingenioso hidalgo don Quijote de la Mancha"** para descubrir qué palabras utilizaba más Cervantes.

### Ejercicio 1: contando palabras

Tienes que implementar lo siguiente:

**1. Fichero de Entrada (Input):**

  * Un único fichero de texto (`quijote.txt`) que contiene la obra completa.

**2. Fase MAP (Mapper):**

El *mapper* debe leer el fichero línea por línea y realizar las siguientes tareas de **normalización** antes de emitir los pares clave-valor:

  - **Conversión a minúsculas:** todas las palabras deben ser convertidas a minúsculas para que "Caballero" y "caballero" cuenten como la misma palabra.
  - **Eliminación de puntuación:** se deben eliminar todos los signos de puntuación (comas, puntos, punto y coma, interrogaciones, etc.). Por ejemplo, "aventura\!" debe tratarse como "aventura".
  - **División (Tokenización):** la línea limpia debe dividirse en palabras individuales (tokens).
  - **Emisión:** por cada palabra válida (token), el mapper debe emitir un par `(palabra, 1)`.

**3. Fase REDUCE (Reducer):**

El *reducer* recibirá una palabra (clave) y una lista de '1's (valores) asociados a esa palabra.

  - **Agregación:** debe sumar todos los valores (los '1's) para obtener el conteo total de esa palabra específica.
  - **Emisión:** debe emitir el par final `(palabra, conteo_total)`.

**4. Fichero de Salida (Output):**

  - El resultado final debe ser un fichero en formato CSV donde cada línea contenga una palabra y su número total de ocurrencias.


```
# Ejemplo de salida esperada:
...
hidalgo, 520
hizo, 310
hombre, 450
...
```

### Ejercicio 2: Filtrado de palabras representativas

En este ejercicio tienes que:

  - Crear una lista (o carga desde un fichero) de palabras no significativas comunes en español (ej. "de", "la", "el", "y", "en", "que", "a", "los", "del", "se").
  - Modificra el *mapper* para que **no emita** ningún par clave-valor si la palabra se encuentra en tu lista de palabras no significativas.
  - El resultado final será un conteo de palabras significativas, excluyendo las más comunes y menos informativas.


### Ejercicio 3: Ordenación por Frecuencia (Top-N)

En este último ejercicio debes:

  - Implementar un segundo trabajo de MapReduce que tome la salida del primero.
  - Este segundo trabajo debe reordenar los datos para que la salida final esté **ordenada por frecuencia de forma descendente**, mostrando las palabras más usadas primero.
  - Tienes que aprovecharte de que Hadoop se encarga de ordenar por la clave, así que en el *mapper* del segundo trabajo deberías invertir el par (clave, valor) para que sea (valor, clave)
  
